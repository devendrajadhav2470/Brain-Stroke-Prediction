{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Brain Stroke Risk Prediction -- Modeling & Results\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook, we build an end-to-end ML pipeline for stroke risk prediction:\n",
        "\n",
        "1. **Data Pipeline**: Load, engineer features, preprocess, and resample.\n",
        "2. **Model Optimization**: Optuna-based hyperparameter tuning for 4 models.\n",
        "3. **Stacking Ensemble**: Meta-learner on top of optimized base models.\n",
        "4. **Threshold Tuning**: Optimize classification threshold for stroke-class F1.\n",
        "5. **SHAP Explainability**: Understand which features drive predictions.\n",
        "6. **MLflow Tracking**: All experiments logged for reproducibility.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root / \"src\"))\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from stroke_risk.data.loader import load_data\n",
        "from stroke_risk.data.preprocessing import build_preprocessor, get_feature_names, resample_data, split_data\n",
        "from stroke_risk.features.engineering import engineer_features\n",
        "from stroke_risk.models.optimize import optimize_all_models\n",
        "from stroke_risk.models.stacking import train_stacking_ensemble\n",
        "from stroke_risk.models.evaluate import (\n",
        "    compute_metrics, find_optimal_threshold, build_comparison_table,\n",
        "    plot_confusion_matrix, plot_roc_curves, plot_precision_recall_curves,\n",
        "    get_classification_report_str, cross_validate_with_ci,\n",
        ")\n",
        "from stroke_risk.explainability.shap_analysis import compute_shap_values, plot_summary, plot_bar, plot_waterfall\n",
        "from stroke_risk.utils.config import load_all_configs\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(name)-30s | %(levelname)-7s | %(message)s\")\n",
        "\n",
        "print(\"All modules loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Pipeline\n",
        "\n",
        "Load data, apply feature engineering, split, preprocess, and resample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configs\n",
        "config = load_all_configs(str(project_root / \"configs\"))\n",
        "data_cfg = config.get(\"data\", {})\n",
        "training_cfg = config.get(\"training\", {})\n",
        "model_cfg = config.get(\"models\", {})\n",
        "\n",
        "# Load and engineer\n",
        "df = load_data()\n",
        "df = engineer_features(df)\n",
        "print(f\"Dataset after feature engineering: {df.shape}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split\n",
        "X_train, X_test, y_train, y_test = split_data(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build preprocessor with engineered features\n",
        "base_cat = data_cfg.get(\"categorical_features\", [])\n",
        "eng_cat = [c for c in [\"age_group\", \"bmi_category\", \"glucose_category\"] if c in X_train.columns]\n",
        "all_cat = base_cat + eng_cat\n",
        "\n",
        "base_num = data_cfg.get(\"numerical_features\", [])\n",
        "eng_num = [c for c in [\"age_x_hypertension\", \"age_x_heart_disease\", \"bmi_x_glucose\", \"age_x_bmi\", \"risk_score\"] if c in X_train.columns]\n",
        "all_num = base_num + eng_num\n",
        "\n",
        "bin_cols = data_cfg.get(\"binary_features\", [])\n",
        "\n",
        "preprocessor = build_preprocessor(categorical_features=all_cat, numerical_features=all_num, binary_features=bin_cols)\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "feature_names = get_feature_names(preprocessor)\n",
        "\n",
        "print(f\"Processed features: {X_train_processed.shape[1]}\")\n",
        "print(f\"Feature names sample: {feature_names[:10]}\")\n",
        "\n",
        "# Resample\n",
        "X_train_res, y_train_res = resample_data(X_train_processed, y_train, strategy=\"smoteenn\")\n",
        "print(f\"\\nAfter resampling: {X_train_res.shape[0]} samples\")\n",
        "print(f\"  Class 0: {(y_train_res == 0).sum()}, Class 1: {(y_train_res == 1).sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Model Optimization with Optuna\n",
        "\n",
        "We optimize 4 models (Logistic Regression, Random Forest, XGBoost, LightGBM) using Bayesian hyperparameter optimization via Optuna's TPE sampler. This is far more efficient than grid search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# Optimize all models (this may take several minutes)\n",
        "optimization_results = optimize_all_models(\n",
        "    X=X_train_res,\n",
        "    y=y_train_res,\n",
        "    model_configs=model_cfg,\n",
        "    scoring=\"f1\",\n",
        "    cv_folds=5,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Show optimization results\n",
        "for name, result in optimization_results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Best CV F1: {result['best_score']:.4f}\")\n",
        "    print(f\"  Best params: {result['best_params']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Stacking Ensemble\n",
        "\n",
        "We combine the 4 optimized models into a stacking ensemble where a Logistic Regression meta-learner learns the optimal way to combine their predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_models = {name: res[\"best_model\"] for name, res in optimization_results.items()}\n",
        "\n",
        "stacking_model = train_stacking_ensemble(\n",
        "    base_models=base_models,\n",
        "    X_train=X_train_res,\n",
        "    y_train=y_train_res,\n",
        "    cv_folds=5,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "all_models = {**base_models, \"stacking_ensemble\": stacking_model}\n",
        "print(f\"Total models to evaluate: {len(all_models)}\")\n",
        "print(f\"Models: {list(all_models.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Threshold Tuning & Evaluation\n",
        "\n",
        "Instead of using the default 0.5 threshold, we optimize the classification threshold for each model to maximize the F1 score on the stroke class. This is critical for imbalanced datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_results = {}\n",
        "\n",
        "for name, model in all_models.items():\n",
        "    y_prob = model.predict_proba(X_test_processed)[:, 1]\n",
        "    \n",
        "    # Find optimal threshold\n",
        "    thresh_result = find_optimal_threshold(y_test.values, y_prob, method=\"f1\")\n",
        "    optimal_threshold = thresh_result[\"threshold\"]\n",
        "    \n",
        "    # Predictions at optimal threshold\n",
        "    y_pred = (y_prob >= optimal_threshold).astype(int)\n",
        "    metrics = compute_metrics(y_test.values, y_pred, y_prob)\n",
        "    \n",
        "    evaluation_results[name] = {\n",
        "        \"model\": model,\n",
        "        \"metrics\": metrics,\n",
        "        \"threshold\": optimal_threshold,\n",
        "        \"y_pred\": y_pred,\n",
        "        \"y_prob\": y_prob,\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  {name} (threshold={optimal_threshold:.4f})\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(get_classification_report_str(y_test.values, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Comparison Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison = build_comparison_table(evaluation_results)\n",
        "comparison.style.background_gradient(subset=[\"F1 (Stroke)\", \"PR-AUC\", \"ROC-AUC\"], cmap=\"YlGn\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROC and Precision-Recall Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_for_plot = {name: res[\"model\"] for name, res in evaluation_results.items()}\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# ROC curves\n",
        "plt.sca(axes[0])\n",
        "roc_fig = plot_roc_curves(models_for_plot, X_test_processed, y_test.values)\n",
        "plt.close(roc_fig)\n",
        "\n",
        "# PR curves\n",
        "plt.sca(axes[1])\n",
        "pr_fig = plot_precision_recall_curves(models_for_plot, X_test_processed, y_test.values)\n",
        "plt.close(pr_fig)\n",
        "\n",
        "# Re-plot on combined figure\n",
        "fig_combined, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
        "\n",
        "for name, model in models_for_plot.items():\n",
        "    RocCurveDisplay.from_estimator(model, X_test_processed, y_test.values, name=name, ax=ax1)\n",
        "ax1.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
        "ax1.set_title(\"ROC Curves\", fontsize=14)\n",
        "ax1.legend(loc=\"lower right\", fontsize=9)\n",
        "\n",
        "for name, model in models_for_plot.items():\n",
        "    PrecisionRecallDisplay.from_estimator(model, X_test_processed, y_test.values, name=name, ax=ax2)\n",
        "prevalence = np.mean(y_test)\n",
        "ax2.axhline(y=prevalence, color=\"k\", linestyle=\"--\", lw=1, label=f\"Baseline ({prevalence:.3f})\")\n",
        "ax2.set_title(\"Precision-Recall Curves\", fontsize=14)\n",
        "ax2.legend(loc=\"upper right\", fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrices -- Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best model confusion matrix\n",
        "best_name = comparison.iloc[0][\"Model\"]\n",
        "best_result = evaluation_results[best_name]\n",
        "\n",
        "print(f\"Best model: {best_name}\")\n",
        "print(f\"Optimal threshold: {best_result['threshold']:.4f}\")\n",
        "print()\n",
        "\n",
        "fig = plot_confusion_matrix(\n",
        "    y_test.values, best_result[\"y_pred\"],\n",
        "    title=f\"Confusion Matrix -- {best_name} (threshold={best_result['threshold']:.3f})\"\n",
        ")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: SHAP Explainability\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) gives us both global feature importance and local, per-prediction explanations. This is essential in healthcare ML where model interpretability is critical for clinical adoption.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute SHAP values for the best model\n",
        "best_model = evaluation_results[best_name][\"model\"]\n",
        "shap_values = compute_shap_values(best_model, X_test_processed, feature_names=feature_names)\n",
        "\n",
        "# Global feature importance -- beeswarm plot\n",
        "fig_summary = plot_summary(shap_values, max_display=15)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global feature importance -- bar plot\n",
        "fig_bar = plot_bar(shap_values, max_display=15)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Local explanation -- waterfall for a stroke-positive prediction\n",
        "stroke_indices = np.where(best_result[\"y_pred\"] == 1)[0]\n",
        "if len(stroke_indices) > 0:\n",
        "    sample_idx = stroke_indices[0]\n",
        "    print(f\"Explaining prediction for sample {sample_idx} (predicted: Stroke)\")\n",
        "    fig_waterfall = plot_waterfall(shap_values, index=sample_idx, max_display=15)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No positive predictions to explain.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "fe_cfg = training_cfg.get(\"feature_engineering\", {})\n",
        "\n",
        "artifact = {\n",
        "    \"model\": best_model,\n",
        "    \"preprocessor\": preprocessor,\n",
        "    \"feature_names\": feature_names,\n",
        "    \"threshold\": best_result[\"threshold\"],\n",
        "    \"model_name\": best_name,\n",
        "    \"feature_engineering_config\": fe_cfg,\n",
        "}\n",
        "\n",
        "models_dir = project_root / \"models\"\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "artifact_path = models_dir / \"best_model.joblib\"\n",
        "joblib.dump(artifact, artifact_path)\n",
        "\n",
        "print(f\"Best model saved to {artifact_path}\")\n",
        "print(f\"  Model: {best_name}\")\n",
        "print(f\"  Threshold: {best_result['threshold']:.4f}\")\n",
        "print(f\"  F1 (Stroke): {best_result['metrics']['f1_stroke']:.4f}\")\n",
        "print(f\"  ROC-AUC: {best_result['metrics'].get('roc_auc', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This project demonstrates a complete, industry-grade ML pipeline:\n",
        "\n",
        "| Component | Technique |\n",
        "|-----------|-----------|\n",
        "| Feature Engineering | Age bins, BMI/glucose categories, interactions, risk score |\n",
        "| Preprocessing | ColumnTransformer (OneHotEncoder + StandardScaler) |\n",
        "| Class Imbalance | SMOTEENN resampling |\n",
        "| HPO | Optuna with TPE sampler |\n",
        "| Ensemble | Stacking (LR meta-learner over 4 base models) |\n",
        "| Threshold Tuning | PR-curve based F1 optimization |\n",
        "| Explainability | SHAP (global + local explanations) |\n",
        "| Experiment Tracking | MLflow |\n",
        "| Deployment | FastAPI + Streamlit + Docker |\n",
        "\n",
        "The model is saved and ready for deployment via `python scripts/train.py` or the FastAPI/Streamlit apps.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
